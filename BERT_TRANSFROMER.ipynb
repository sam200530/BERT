{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/eAgaUwfH/J9i9CuMAbJy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c1f73614f4c4a24a03a2797c3e249ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_512ba60762904562ab9c6c315035bd4b",
              "IPY_MODEL_487b6b57aa0b4297beff04ff5abaceb7",
              "IPY_MODEL_72c002973033431ab44e2b997541f04f"
            ],
            "layout": "IPY_MODEL_49d278ca57e8456a9d0fe4cdb8a1b887"
          }
        },
        "512ba60762904562ab9c6c315035bd4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce5adbe7f4cc410793c60b84b861a307",
            "placeholder": "​",
            "style": "IPY_MODEL_f4ffb4e181c2467893c0ab39696d37ed",
            "value": "model.safetensors: 100%"
          }
        },
        "487b6b57aa0b4297beff04ff5abaceb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f725ab94e043279a29c9e2c755e461",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78af794e212540448889d000db35cdfb",
            "value": 440449768
          }
        },
        "72c002973033431ab44e2b997541f04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b12ead9c1dde4b01b7e0ebd7e12813e8",
            "placeholder": "​",
            "style": "IPY_MODEL_420c38719bd540fdad530ecf514401e2",
            "value": " 440M/440M [00:09&lt;00:00, 48.3MB/s]"
          }
        },
        "49d278ca57e8456a9d0fe4cdb8a1b887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5adbe7f4cc410793c60b84b861a307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ffb4e181c2467893c0ab39696d37ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f725ab94e043279a29c9e2c755e461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78af794e212540448889d000db35cdfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b12ead9c1dde4b01b7e0ebd7e12813e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420c38719bd540fdad530ecf514401e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam200530/BERT/blob/main/BERT_TRANSFROMER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK_1"
      ],
      "metadata": {
        "id": "V3je7boILE9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uvvShUA9Em1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "yrA2lW4QEnsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "!pip install transformers torch pandas scikit-learn numpy tqdm -q\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Multi-Head Self-Attention Implementation\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim divisible by num_heads\"\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn, V)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "        output = self.out(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"Multi-Head Self-Attention Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Nq69quEp8o",
        "outputId": "1a100591-de42-4946-ec22-c15f08b83d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Self-Attention Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim=2048, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "print(\" Feed-Forward Layer \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v57-FIqEEybV",
        "outputId": "640222c0-fbeb-40bd-b7c7-4c94701ae2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feed-Forward Layer \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim=2048, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"Transformer Encoder Layer \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuMoXnlOE909",
        "outputId": "4c946c11-ccb9-4679-cdc9-b987027ce61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer Encoder Layer \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=768, num_heads=12, num_layers=2,\n",
        "                 max_seq_len=512, num_classes=2, dropout=0.1):\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, embed_dim * 4, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Linear(embed_dim, embed_dim)\n",
        "        self.pooler_activation = nn.Tanh()\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        token_embeds = self.token_embedding(input_ids)\n",
        "\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        pos_embeds = self.position_embedding(positions)\n",
        "\n",
        "        embeddings = token_embeds + pos_embeds\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        hidden_states = embeddings\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            hidden_states = encoder_layer(hidden_states, attention_mask)\n",
        "\n",
        "        pooled_output = self.pooler(hidden_states[:, 0])\n",
        "        pooled_output = self.pooler_activation(pooled_output)\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return {\n",
        "            'token_embeddings': token_embeds,\n",
        "            'positional_embeddings': pos_embeds,\n",
        "            'attention_output': hidden_states,\n",
        "            'pooled_output': pooled_output,\n",
        "            'logits': logits\n",
        "        }\n",
        "\n",
        "print(\"  BERT Model \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK-d8oJSFANk",
        "outputId": "f46fb61b-401f-4631-8db5-71592e760670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  BERT Model \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(f\" Tokenizer (vocab size: {tokenizer.vocab_size})\")\n",
        "\n",
        "# BERT with Sample Sentence\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "model = BertModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=768,\n",
        "    num_heads=12,\n",
        "    num_layers=2,\n",
        "    max_seq_len=512,\n",
        "    num_classes=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "test_sentence = \"This is a comprehensive test sentence with more than ten words to verify the BERT model implementation\"\n",
        "print(f\"\\nTest sentence ({len(test_sentence.split())} words): {test_sentence}\")\n",
        "\n",
        "encoded = tokenizer(test_sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "input_ids = encoded['input_ids'].to(device)\n",
        "attention_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "print(f\"\\nTokenized input shape: {input_ids.shape}\")\n",
        "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
        "print(\"Segmentation info: All tokens assigned to segment 0 (for single sentence)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cu0Ps7gFNvq",
        "outputId": "530c351b-57f5-4d06-f54c-66ad5bd50dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokenizer (vocab size: 30522)\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "Test sentence (17 words): This is a comprehensive test sentence with more than ten words to verify the BERT model implementation\n",
            "\n",
            "Tokenized input shape: torch.Size([1, 19])\n",
            "Tokens: ['[CLS]', 'this', 'is', 'a', 'comprehensive', 'test', 'sentence', 'with', 'more', 'than', 'ten', 'words', 'to', 'verify', 'the', 'bert', 'model', 'implementation', '[SEP]']\n",
            "Segmentation info: All tokens assigned to segment 0 (for single sentence)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n1. Token Embeddings Shape: {outputs['token_embeddings'].shape}\")\n",
        "print(f\"   (batch_size, sequence_length, embedding_dim)\")\n",
        "\n",
        "print(f\"\\n2. Positional Embeddings Shape: {outputs['positional_embeddings'].shape}\")\n",
        "print(f\"   (batch_size, sequence_length, embedding_dim)\")\n",
        "\n",
        "print(f\"\\n3. Output of Multi-Head Attention Shape: {outputs['attention_output'].shape}\")\n",
        "print(f\"   (batch_size, sequence_length, embedding_dim)\")\n",
        "\n",
        "print(f\"\\n4. Output of Feed-Forward Layer Shape: {outputs['attention_output'].shape}\")\n",
        "print(f\"   (same as attention output after residual connection)\")\n",
        "\n",
        "print(f\"\\n5. Output of Transformer Encoder Layer Shape: {outputs['attention_output'].shape}\")\n",
        "print(f\"   (batch_size, sequence_length, embedding_dim)\")\n",
        "\n",
        "print(f\"\\n6. Output Probabilities from Classifier Shape: {outputs['logits'].shape}\")\n",
        "print(f\"   (batch_size, num_classes)\")\n",
        "\n",
        "\n",
        "print(\"SHAPE OF EACH PARAMETER \")\n",
        "\n",
        "total_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name:60s}: {str(param.shape):30s} ({param.numel():,} params)\")\n",
        "    total_params += param.numel()\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVPAhYIZFYOS",
        "outputId": "156e313f-8375-4412-e227-33f26d92528d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Token Embeddings Shape: torch.Size([1, 19, 768])\n",
            "   (batch_size, sequence_length, embedding_dim)\n",
            "\n",
            "2. Positional Embeddings Shape: torch.Size([1, 19, 768])\n",
            "   (batch_size, sequence_length, embedding_dim)\n",
            "\n",
            "3. Output of Multi-Head Attention Shape: torch.Size([1, 19, 768])\n",
            "   (batch_size, sequence_length, embedding_dim)\n",
            "\n",
            "4. Output of Feed-Forward Layer Shape: torch.Size([1, 19, 768])\n",
            "   (same as attention output after residual connection)\n",
            "\n",
            "5. Output of Transformer Encoder Layer Shape: torch.Size([1, 19, 768])\n",
            "   (batch_size, sequence_length, embedding_dim)\n",
            "\n",
            "6. Output Probabilities from Classifier Shape: torch.Size([1, 2])\n",
            "   (batch_size, num_classes)\n",
            "SHAPE OF EACH PARAMETER \n",
            "token_embedding.weight                                      : torch.Size([30522, 768])       (23,440,896 params)\n",
            "position_embedding.weight                                   : torch.Size([512, 768])         (393,216 params)\n",
            "encoder_layers.0.attention.query.weight                     : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.0.attention.query.bias                       : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.attention.key.weight                       : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.0.attention.key.bias                         : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.attention.value.weight                     : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.0.attention.value.bias                       : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.attention.out.weight                       : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.0.attention.out.bias                         : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.feed_forward.linear1.weight                : torch.Size([3072, 768])        (2,359,296 params)\n",
            "encoder_layers.0.feed_forward.linear1.bias                  : torch.Size([3072])             (3,072 params)\n",
            "encoder_layers.0.feed_forward.linear2.weight                : torch.Size([768, 3072])        (2,359,296 params)\n",
            "encoder_layers.0.feed_forward.linear2.bias                  : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.norm1.weight                               : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.norm1.bias                                 : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.norm2.weight                               : torch.Size([768])              (768 params)\n",
            "encoder_layers.0.norm2.bias                                 : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.attention.query.weight                     : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.1.attention.query.bias                       : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.attention.key.weight                       : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.1.attention.key.bias                         : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.attention.value.weight                     : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.1.attention.value.bias                       : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.attention.out.weight                       : torch.Size([768, 768])         (589,824 params)\n",
            "encoder_layers.1.attention.out.bias                         : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.feed_forward.linear1.weight                : torch.Size([3072, 768])        (2,359,296 params)\n",
            "encoder_layers.1.feed_forward.linear1.bias                  : torch.Size([3072])             (3,072 params)\n",
            "encoder_layers.1.feed_forward.linear2.weight                : torch.Size([768, 3072])        (2,359,296 params)\n",
            "encoder_layers.1.feed_forward.linear2.bias                  : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.norm1.weight                               : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.norm1.bias                                 : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.norm2.weight                               : torch.Size([768])              (768 params)\n",
            "encoder_layers.1.norm2.bias                                 : torch.Size([768])              (768 params)\n",
            "pooler.weight                                               : torch.Size([768, 768])         (589,824 params)\n",
            "pooler.bias                                                 : torch.Size([768])              (768 params)\n",
            "classifier.weight                                           : torch.Size([2, 768])           (1,536 params)\n",
            "classifier.bias                                             : torch.Size([2])                (2 params)\n",
            "\n",
            "Total parameters: 38,601,986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nw4_1fjKGb0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK_2\n"
      ],
      "metadata": {
        "id": "X_YLuFEfLSzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "s8HJd_4IGb6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/nlp/\"\n",
        "\n",
        "\n",
        "train_path = base_path + 'all_train.tsv'\n",
        "test_path = base_path + 'all_test_public.tsv'\n",
        "validate_path = base_path + 'all_validate.tsv'\n",
        "\n",
        "df_train = pd.read_csv(train_path, sep='\\t')\n",
        "df_test = pd.read_csv(test_path, sep='\\t')\n",
        "df_validate = pd.read_csv(validate_path, sep='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnjL5gXgG8cV",
        "outputId": "4bf47ec1-b73f-4892-8316-20775577bfda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Train shape: {df_train.shape}\")\n",
        "print(f\" Test shape: {df_test.shape}\")\n",
        "print(f\" Validate shape: {df_validate.shape}\")\n",
        "print(f\"\\nColumns available: {df_train.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW5W4MSzHIjA",
        "outputId": "d07c4cd6-9512-473a-8444-6fbe5e59f66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train shape: (878218, 20)\n",
            " Test shape: (92444, 20)\n",
            " Validate shape: (92444, 20)\n",
            "\n",
            "Columns available: ['Unnamed: 0.2', 'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'author', 'clean_title', 'created_utc', 'domain', 'hasImage', 'id', 'image_url', 'linked_submission_id', 'num_comments', 'score', 'subreddit', 'title', 'upvote_ratio', '2_way_label', '3_way_label', '6_way_label']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "required_cols = ['clean_title', '2_way_label', 'id']\n",
        "df_train = df_train[required_cols].copy()\n",
        "df_test = df_test[required_cols].copy()\n",
        "df_validate = df_validate[required_cols].copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "8GVjLybdHsq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nColumns available: {df_train.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaYvWrrgH2HQ",
        "outputId": "31c8663e-1f0b-44d3-dbe1-80e17f858725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Columns available: ['clean_title', '2_way_label', 'id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop_duplicates(subset=['clean_title'])\n",
        "df_test = df_test.drop_duplicates(subset=['clean_title'])\n",
        "df_validate = df_validate.drop_duplicates(subset=['clean_title'])"
      ],
      "metadata": {
        "id": "EfiOcZKnH3ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/all/\"\n",
        "\n",
        "comments_path = base_path + 'all_comments.tsv'\n",
        "df_comments = pd.read_csv(comments_path, sep='\\t')\n",
        "\n",
        "required_comment_cols = ['id', 'submission_id', 'body', 'parent_id']\n",
        "df_comments = df_comments[required_comment_cols].dropna()\n",
        "\n",
        "print(f\"\\n Comments loaded: {len(df_comments):,}\")\n",
        "print(f\"Columns: {df_comments.columns.tolist()}\")\n",
        "print(f\"\\nSample comment structure:\")\n",
        "print(df_comments.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxPAlO6jIHgN",
        "outputId": "b9bee498-e9ce-4128-edd6-fd0ba16bc150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-653497297.py:7: DtypeWarning: Columns (0,1,2,3,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_comments = pd.read_csv(comments_path, sep='\\t')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Comments loaded: 10,000,647\n",
            "Columns: ['id', 'submission_id', 'body', 'parent_id']\n",
            "\n",
            "Sample comment structure:\n",
            "        id submission_id                                               body  \\\n",
            "0  f4deplg        dkdml1  Scroll, scroll, scroll.  Pause.  Scroll back u...   \n",
            "1  f4d79bi        dkdml1  A lot of the people who felt quite strongly ab...   \n",
            "2  f4ddmlk        dkdml1  T H E   S P H E R E   S H A L L   R I S E   A ...   \n",
            "\n",
            "   parent_id  \n",
            "0  t3_dkdml1  \n",
            "1  t3_dkdml1  \n",
            "2  t3_dkdml1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_comments_to_posts(df_posts, df_comments):\n",
        "\n",
        "    top_level = df_comments[df_comments['parent_id'].str.startswith('t3_', na=False)].copy()\n",
        "\n",
        "    comments_grouped = top_level.groupby('submission_id')['body'].apply(\n",
        "        lambda x: ' [SEP] '.join(x.head(3))\n",
        "    ).to_dict()\n",
        "\n",
        "    df_posts['text'] = df_posts.apply(\n",
        "        lambda row: f\"{row['clean_title']} [SEP] {comments_grouped.get(row['id'], '')}\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df_posts\n",
        "\n",
        "df_train = add_comments_to_posts(df_train, df_comments)\n",
        "df_test = add_comments_to_posts(df_test, df_comments)\n",
        "df_validate = add_comments_to_posts(df_validate, df_comments)\n",
        "\n",
        "print(\"\\n Comments add done\")\n",
        "print(\"Format used: [POST_TITLE] [SEP] [COMMENT1] [SEP] [COMMENT2] [SEP] [COMMENT3]\")\n",
        "print(\"  - Prefix t3_ → Comment directly on post (top-level)\")\n",
        "print(\"  - Prefix t1_ → Reply to another comment\")\n",
        "print(f\"\\nSample enriched text:\\n{df_train['text'].iloc[0][:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akd1rr87J_zW",
        "outputId": "59c35e14-50d3-4a5e-bc74-27cf0779d7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Comments add done\n",
            "Format used: [POST_TITLE] [SEP] [COMMENT1] [SEP] [COMMENT2] [SEP] [COMMENT3]\n",
            "  - Prefix t3_ → Comment directly on post (top-level)\n",
            "  - Prefix t1_ → Reply to another comment\n",
            "\n",
            "Sample enriched text:\n",
            "my walgreens offbrand mucinex was engraved with the letters mucinex but in a different order [SEP] Does it help with Dyslexia?...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_original = pd.concat([df_train, df_test, df_validate])\n",
        "\n",
        "print(f\"\\n Total number of posts in original dataset: {len(df_all_original):,}\")\n",
        "\n",
        "print(f\"\\n Distribution:\")\n",
        "dist = df_all_original['2_way_label'].value_counts()\n",
        "print(f\"   Fake posts (label=1): {dist.get(1, 0):,}\")\n",
        "print(f\"   Non-fake posts (label=0): {dist.get(0, 0):,}\")\n",
        "\n",
        "comment_counts = df_comments.groupby('submission_id').size()\n",
        "df_all_original['num_comments'] = df_all_original['id'].map(comment_counts).fillna(0)\n",
        "\n",
        "posts_with_comments = (df_all_original['num_comments'] > 0).sum()\n",
        "print(f\"\\n Number of posts with at least one comment: {posts_with_comments:,}\")\n",
        "\n",
        "print(f\"\\n Mean and Std of comments:\")\n",
        "fake_comments = df_all_original[df_all_original['2_way_label'] == 1]['num_comments']\n",
        "nonfake_comments = df_all_original[df_all_original['2_way_label'] == 0]['num_comments']\n",
        "\n",
        "print(f\"   Fake posts - Mean: {fake_comments.mean():.2f}, Std: {fake_comments.std():.2f}\")\n",
        "print(f\"   Non-fake posts - Mean: {nonfake_comments.mean():.2f}, Std: {nonfake_comments.std():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqx_r_-WIk3i",
        "outputId": "9c214fe9-3a4f-4740-a5c4-591c7f6f4c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "a. Total number of posts in original dataset: 880,437\n",
            "\n",
            "b. Distribution:\n",
            "   Fake posts (label=1): 463,755\n",
            "   Non-fake posts (label=0): 416,682\n",
            "\n",
            "c. Number of posts with at least one comment: 556,802\n",
            "\n",
            "d. Mean and Std of comments:\n",
            "   Fake posts - Mean: 14.65, Std: 55.41\n",
            "   Non-fake posts - Mean: 5.08, Std: 20.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def balance_dataset(df, min_samples=2500):\n",
        "\n",
        "    fake = df[df['2_way_label'] == 1]\n",
        "    nonfake = df[df['2_way_label'] == 0]\n",
        "\n",
        "    n_samples = max(min_samples, min(len(fake), len(nonfake)))\n",
        "\n",
        "    fake_sampled = fake.sample(n=n_samples, random_state=42)\n",
        "    nonfake_sampled = nonfake.sample(n=n_samples, random_state=42)\n",
        "\n",
        "    balanced_df = pd.concat([fake_sampled, nonfake_sampled]).sample(frac=1, random_state=42)\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "df_train_balanced = balance_dataset(df_train, min_samples=2500)\n",
        "df_test_balanced = balance_dataset(df_test, min_samples=2500)\n",
        "\n",
        "\n",
        "print(f\"Balanced Train: {len(df_train_balanced):,}\")\n",
        "print(f\"  Fake: {(df_train_balanced['2_way_label']==1).sum():,}\")\n",
        "print(f\"  Non-fake: {(df_train_balanced['2_way_label']==0).sum():,}\")\n",
        "\n",
        "print(f\"\\nBalanced Test: {len(df_test_balanced):,}\")\n",
        "print(f\"  Fake: {(df_test_balanced['2_way_label']==1).sum():,}\")\n",
        "print(f\"  Non-fake: {(df_test_balanced['2_way_label']==0).sum():,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDE75YstJjgB",
        "outputId": "02e1aa9f-2048-4ed8-b049-e40aa97a2f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Train: 679,240\n",
            "  Fake: 339,620\n",
            "  Non-fake: 339,620\n",
            "\n",
            "Balanced Test: 77,030\n",
            "  Fake: 38,515\n",
            "  Non-fake: 38,515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df_train_balanced,\n",
        "    test_size=0.2,\n",
        "    stratify=df_train_balanced['2_way_label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Training set: {len(train_df):,}\")\n",
        "print(f\"  Fake: {(train_df['2_way_label']==1).sum():,}\")\n",
        "print(f\"  Non-fake: {(train_df['2_way_label']==0).sum():,}\")\n",
        "\n",
        "print(f\"\\nValidation set: {len(val_df):,}\")\n",
        "print(f\"  Fake: {(val_df['2_way_label']==1).sum():,}\")\n",
        "print(f\"  Non-fake: {(val_df['2_way_label']==0).sum():,}\")\n",
        "\n",
        "\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "df_train[['text', '2_way_label']].to_csv(os.path.join(OUTPUT_DIR, 'train.csv'), index=False)\n",
        "df_test[['text', '2_way_label']].to_csv(os.path.join(OUTPUT_DIR, 'test.csv'), index=False)\n",
        "print(f\" Train set saved: {len(df_train)} posts (80%)\")\n",
        "print(f\" Test set saved: {len(df_test)} posts (20%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijayn5cZLOF9",
        "outputId": "4adddbc7-4177-4f39-9237-a2d407d28a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 543,392\n",
            "  Fake: 271,696\n",
            "  Non-fake: 271,696\n",
            "\n",
            "Validation set: 135,848\n",
            "  Fake: 67,924\n",
            "  Non-fake: 67,924\n",
            " Train set saved: 719696 posts (80%)\n",
            " Test set saved: 80433 posts (20%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIg0zp6-Ut0M",
        "outputId": "3a116bf4-db4d-464c-971a-abcef3c182d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced_all = pd.concat([df_train_balanced, df_test_balanced])\n",
        "\n",
        "print(f\"\\n Number of fake posts: {(df_balanced_all['2_way_label']==1).sum():,}\")\n",
        "print(f\"   Number of non-fake posts: {(df_balanced_all['2_way_label']==0).sum():,}\")\n",
        "\n",
        "print(f\"\\n Distribution of posts in train set:\")\n",
        "print(f\"   Fake: {(train_df['2_way_label']==1).sum():,}\")\n",
        "print(f\"   Non-fake: {(train_df['2_way_label']==0).sum():,}\")\n",
        "\n",
        "print(f\"\\n Distribution of posts in test set:\")\n",
        "print(f\"   Fake: {(df_test_balanced['2_way_label']==1).sum():,}\")\n",
        "print(f\"   Non-fake: {(df_test_balanced['2_way_label']==0).sum():,}\")\n",
        "\n",
        "train_df['num_comments'] = train_df['id'].map(comment_counts).fillna(0)\n",
        "df_test_balanced['num_comments'] = df_test_balanced['id'].map(comment_counts).fillna(0)\n",
        "\n",
        "print(f\"\\n Comment statistics in balanced dataset:\")\n",
        "fake_bal = train_df[train_df['2_way_label'] == 1]['num_comments']\n",
        "nonfake_bal = train_df[train_df['2_way_label'] == 0]['num_comments']\n",
        "\n",
        "print(f\"   Fake posts - Mean: {fake_bal.mean():.2f}, Std: {fake_bal.std():.2f}\")\n",
        "print(f\"   Non-fake posts - Mean: {nonfake_bal.mean():.2f}, Std: {nonfake_bal.std():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9R4IvL2LXDl",
        "outputId": "74514fdf-811e-4f81-c689-9652b8c035ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Number of fake posts: 378,135\n",
            "   Number of non-fake posts: 378,135\n",
            "\n",
            " Distribution of posts in train set:\n",
            "   Fake: 271,696\n",
            "   Non-fake: 271,696\n",
            "\n",
            " Distribution of posts in test set:\n",
            "   Fake: 38,515\n",
            "   Non-fake: 38,515\n",
            "\n",
            " Comment statistics in balanced dataset:\n",
            "   Fake posts - Mean: 14.49, Std: 55.00\n",
            "   Non-fake posts - Mean: 5.09, Std: 20.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kEpLTwFsLm5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK_3\n"
      ],
      "metadata": {
        "id": "29MOcAUFLuEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "nFIte_LPLm_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FakedditDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\" Dataset class created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY9exhP1Lo5u",
        "outputId": "7083b532-c04d-48fd-c09e-a6ddfee4067a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset class created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = FakedditDataset(\n",
        "    train_df['text'].values,\n",
        "    train_df['2_way_label'].values,\n",
        "    tokenizer,\n",
        "    MAX_LENGTH\n",
        ")\n",
        "\n",
        "val_dataset = FakedditDataset(\n",
        "    val_df['text'].values,\n",
        "    val_df['2_way_label'].values,\n",
        "    tokenizer,\n",
        "    MAX_LENGTH\n",
        ")\n",
        "\n",
        "test_dataset = FakedditDataset(\n",
        "    df_test_balanced['text'].values,\n",
        "    df_test_balanced['2_way_label'].values,\n",
        "    tokenizer,\n",
        "    MAX_LENGTH\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\" Data loaders created\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "print(f\"\\nUsing max_length={MAX_LENGTH} tokens (subset of input)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2ebiC5cMg1s",
        "outputId": "30fef7ca-9c86-4130-9876-f3104b54285f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data loaders created\n",
            "  Train batches: 33962\n",
            "  Val batches: 8491\n",
            "  Test batches: 4815\n",
            "\n",
            "Using max_length=128 tokens (subset of input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "print(f\" Pre-trained BERT model loaded on {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "5c1f73614f4c4a24a03a2797c3e249ce",
            "512ba60762904562ab9c6c315035bd4b",
            "487b6b57aa0b4297beff04ff5abaceb7",
            "72c002973033431ab44e2b997541f04f",
            "49d278ca57e8456a9d0fe4cdb8a1b887",
            "ce5adbe7f4cc410793c60b84b861a307",
            "f4ffb4e181c2467893c0ab39696d37ed",
            "c8f725ab94e043279a29c9e2c755e461",
            "78af794e212540448889d000db35cdfb",
            "b12ead9c1dde4b01b7e0ebd7e12813e8",
            "420c38719bd540fdad530ecf514401e2"
          ]
        },
        "id": "yVZ4T-b3MjJL",
        "outputId": "1d8761af-0d5f-483b-c813-0103e4ea2217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c1f73614f4c4a24a03a2797c3e249ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pre-trained BERT model loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_STEPS = 100\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Optimizer: AdamW\")\n",
        "print(f\"Loss function: CrossEntropyLoss (on training set)\")\n",
        "print(f\"Warmup steps: {WARMUP_STEPS}\")\n",
        "print(f\"Total training steps: {total_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNxVAyUhMtmU",
        "outputId": "3df11493-a95d-4e60-ee44-2acb8a906a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1\n",
            "Batch size: 16\n",
            "Learning rate: 2e-05\n",
            "Optimizer: AdamW\n",
            "Loss function: CrossEntropyLoss (on training set)\n",
            "Warmup steps: 100\n",
            "Total training steps: 33962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc='Training')\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return avg_loss, predictions, true_labels\n"
      ],
      "metadata": {
        "id": "sVs-rAOBMz_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "print(\"Evaluation function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edTnU6Z5M42o",
        "outputId": "2fc5e76d-de7b-4ac2-9daa-edd04537d4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_preds, train_labels = train_epoch(\n",
        "        model, train_loader, optimizer, scheduler, device\n",
        "    )\n",
        "\n",
        "    val_preds, val_labels = evaluate(model, val_loader, device)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    print(f'\\nTrain Loss: {train_loss:.4f}')\n",
        "    print(f'Val Accuracy: {val_acc:.4f}')\n",
        "    print(f'Epoch Time: {epoch_time:.2f}s')\n",
        "\n",
        "print(\"\\n Training done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XUEkol8M6or",
        "outputId": "7ab7f459-38f2-4857-d18d-0aa76218b0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 33962/33962 [3:24:58<00:00,  2.76it/s, loss=0.112]\n",
            "Evaluating: 100%|██████████| 8491/8491 [17:15<00:00,  8.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2082\n",
            "Val Accuracy: 0.9368\n",
            "Epoch Time: 13333.68s\n",
            "\n",
            " Training done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds, test_labels = evaluate(model, test_loader, device)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, test_preds)\n",
        "precision = precision_score(test_labels, test_preds, average='binary')\n",
        "recall = recall_score(test_labels, test_preds, average='binary')\n",
        "f1 = f1_score(test_labels, test_preds, average='binary')\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "\n",
        "print(\"METRICS  FOR TEST SET\")\n",
        "print(f\"{'Metric':<20} {'Value':<10}\")\n",
        "print(f\"{'Accuracy':<20} {accuracy:.4f}\")\n",
        "print(f\"{'Precision':<20} {precision:.4f}\")\n",
        "print(f\"{'Recall':<20} {recall:.4f}\")\n",
        "print(f\"{'F1-Score':<20} {f1:.4f}\")\n",
        "\n",
        "\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"                 0      1\")\n",
        "print(f\"Actual   0     {cm[0][0]:5d}  {cm[0][1]:5d}\")\n",
        "print(f\"         1     {cm[1][0]:5d}  {cm[1][1]:5d}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsVoy56nKLgs",
        "outputId": "66209667-229c-4538-d126-d422600e3ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 4815/4815 [10:52<00:00,  7.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METRICS  FOR TEST SET\n",
            "Metric               Value     \n",
            "Accuracy             0.9384\n",
            "Precision            0.9444\n",
            "Recall               0.9316\n",
            "F1-Score             0.9380\n",
            "CONFUSION MATRIX\n",
            "                 Predicted\n",
            "                 0      1\n",
            "Actual   0     36403   2112\n",
            "         1      2633  35882\n"
          ]
        }
      ]
    }
  ]
}